{
 "metadata": {
  "name": "",
  "signature": "sha256:324fd5a24663e81a5049685181d1d88be8992c44d747a1ae3368e739e8ca0cd9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import os\n",
      "import json\n",
      "import pandas as pd\n",
      "import collections\n",
      "\n",
      "#Specify directory to data folder below.\n",
      "mainDir = '/home/ubuntu/'\n",
      "dataDir = mainDir + 'DeepMile/assignment/OCEAN_Social_Data'\n",
      "os.chdir(dataDir)\n",
      "\n",
      "#Useful debugging functions, From \"Python for Data Analysis\" P.65\n",
      "def set_trace():\n",
      "    from IPython.core.debugger import Pdb\n",
      "    Pdb(color_scheme='Linux').set_trace(sys._getframe().f_back)\n",
      "    \n",
      "def debug(f, *args, **kwargs):\n",
      "    from IPython.core.debugger import Pdb\n",
      "    pdb = Pdb(color_scheme='Linux')\n",
      "    return pdb.runcall(f,*args,**kwargs)\n",
      "\n",
      "\n",
      "#http://stackoverflow.com/questions/6027558/flatten-nested-python-dictionaries-compressing-keys\n",
      "def flatten(d, parent_key='', sep='_'):\n",
      "    items = []\n",
      "    for k, v in d.items():\n",
      "        new_key = parent_key + sep + k if parent_key else k\n",
      "        if isinstance(v, collections.MutableMapping):\n",
      "            items.extend(flatten(v, new_key).items())\n",
      "        else:\n",
      "            items.append((new_key, v))\n",
      "    return dict(items)\n",
      "\n",
      "\n",
      "def collapse_raw_features(group):\n",
      "    gender = group['demographic_gender'][group.index[0]]\n",
      "    klout_score = group['klout_score'].max()\n",
      "    followers_count = group['twitter_user_followers_count'].max()\n",
      "    favorites_count = group['twitter_user_favourites_count'].max()\n",
      "    return pd.Series({'gender' : gender, \n",
      "                      'klout_score' : klout_score, \n",
      "                      'followers_count' : followers_count, \n",
      "                      'favorites_count' : favorites_count})\n",
      "\n",
      "def concatenate_tweets(group):\n",
      "    tweet_doc = ', '\n",
      "    tweet_doc = tweet_doc.join(group['interaction_content'])\n",
      "    return tweet_doc\n",
      "\n",
      "def get_profiles(group):\n",
      "    return group['twitter_user_description'][group.index[0]]\n",
      "\n",
      "def sentiment_count(text):\n",
      "    text = text.lower()\n",
      "    words = text.split(' ')\n",
      "    positive_count, negative_count = 0,0\n",
      "    for word in words:\n",
      "       if word in positive_words:\n",
      "            positive_count += 1\n",
      "       if word in negative_words:\n",
      "            negative_count += 1\n",
      "    return pd.Series({'positive_count' : positive_count, 'negative_count' : negative_count})\n",
      "\n",
      "def get_sum_hashtags_and_mentions(group):\n",
      "    hashtag_count = group['len_interaction_hashtags'].sum()\n",
      "    user_mention_count = group['len_interaction_mentions'].sum()\n",
      "    return pd.Series({'hashtag_count' : hashtag_count, 'user_mention_count' : user_mention_count})\n",
      "\n",
      "#Load twitter data\n",
      "file_list = os.listdir(dataDir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "?json.loads"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Object `json.loads` not found.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "temp_list = []\n",
      "for file_path in file_list:   \n",
      "    with open (file_path, \"r\") as myfile:\n",
      "        temp_data = myfile.readlines()\n",
      "    for line in temp_data:\n",
      "        try:\n",
      "            temp = json.loads(line)\n",
      "        except ValueError:\n",
      "            next\n",
      "        temp_list.append(flatten(temp))\n",
      "\n",
      "\n",
      "data = pd.DataFrame(temp_list)\n",
      "\n",
      "#Load positive and negative word lists to be used for sentiment analysis of twitter posts and user profile.\n",
      "\n",
      "positive_words_path = mainDir + '/DeepMile/assignment/positive-words.txt'\n",
      "negative_words_path = mainDir + '/DeepMile/assignment/negative-words.txt'\n",
      "\n",
      "with open(positive_words_path) as f:\n",
      "    lines1 = f.readlines()\n",
      "\n",
      "with open(negative_words_path) as f:\n",
      "    lines2 = f.readlines()\n",
      "    \n",
      "positive_words = [x.strip('\\n') for x in lines1]\n",
      "negative_words = [x.strip('\\n') for x in lines2]\n",
      "\n",
      "\n",
      "#Two fields in the data are feasible for text processing:  \n",
      "#user tweets (interaction_content) and user profile (twitter_user_description).\n",
      "#We summarize the sentiment of these fields by counting the number of positive or negative words.\n",
      "#We implement this from predetermined list of positive and negative words (thanks John for the lists).\n",
      "\n",
      "#There is some processing to the data that must be done first.\n",
      "#Each user may have multiple tweets. First we must concatenate the tweets into a single 'document'.\n",
      "tweets = data[['interaction_author_username', 'interaction_content']].groupby('interaction_author_username').apply(concatenate_tweets)\n",
      "\n",
      "#We assume the user profile is a static among different tweets by a user. \n",
      "#This assumption is reasonable if the user doesn't edit their profile too frequently.\n",
      "#We extract the first occurence of a profile from the data.\n",
      "#Some profiles are left blank. We denote blank profiles by a '' value.\n",
      "profiles = data[['interaction_author_username', 'twitter_user_description']].groupby('interaction_author_username').apply(get_profiles)\n",
      "profiles = profiles.fillna('')\n",
      "\n",
      "#Calculate the positive and negative sentiments of the profiles and tweets.\n",
      "tweet_sentiments = tweets.apply(sentiment_count)\n",
      "profile_sentiments = profiles.apply(sentiment_count)\n",
      "#Rename columns for organization.\n",
      "tweet_sentiments.columns = ['tweet_negative_word_count', 'tweet_positive_word_count']\n",
      "profile_sentiments.columns = ['profile_negative_word_count', 'profile_positive_word_count']\n",
      "\n",
      "\n",
      "#Note following warning arises due to the presence of unicode characters in the tweets:\n",
      "#-c:39: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "\n",
      "\n",
      "#Compute number of hashtags mentioned and number of other users mentioned for each user.\n",
      "#The number of other users mentioned in a tweets gives an indication of how social the tweeting user is.\n",
      "#Likewise, for the number of hashtags mentioned. \n",
      "mentions_and_hashtags = data[['interaction_author_username', 'interaction_hashtags', 'interaction_mentions']].fillna('')\n",
      "mentions_and_hashtags['len_interaction_hashtags'] = mentions_and_hashtags['interaction_hashtags'].apply(len)\n",
      "mentions_and_hashtags['len_interaction_mentions'] = mentions_and_hashtags['interaction_mentions'].apply(len)\n",
      "\n",
      "temp = mentions_and_hashtags[['interaction_author_username', 'len_interaction_hashtags', 'len_interaction_mentions']]\n",
      "count_mentions_and_hashtags = temp.groupby('interaction_author_username').apply(get_sum_hashtags_and_mentions)\n",
      "\n",
      "\n",
      "#Compute character length of user description, and interaction content.\n",
      "profile_lengths = pd.DataFrame(profiles.apply(len), columns=[\"nchar_profile\"])\n",
      "tweet_lengths = pd.DataFrame(tweets.apply(len), columns=[\"nchar_tweets\"])\n",
      "\n",
      "\n",
      "#Collapse data across different tweets by the same user.\n",
      "#These variables ought to be the same across different tweets for a user.\n",
      "raw_features = data[['interaction_author_username','demographic_gender', \n",
      "                   'twitter_user_favourites_count', 'twitter_user_followers_count', 'twitter_user_friends_count', 'klout_score']]\n",
      "collapsed_raw_features = raw_features.groupby('interaction_author_username').apply(collapse_raw_features)\n",
      "collapsed_raw_features['gender'] = collapsed_raw_features['gender'].fillna(0)\n",
      "\n",
      "#Fill in missing Klout Scores with 0\n",
      "collapsed_raw_features['klout_score'] = collapsed_raw_features['klout_score'].fillna(0)\n",
      "\n",
      "#Merge predictor variables. Then merge all variables into a regression table.\n",
      "predictors = pd.concat([tweet_sentiments, profile_sentiments, count_mentions_and_hashtags, collapsed_raw_features, profile_lengths, tweet_lengths], axis=1)Kz\n",
      "response = pd.read_csv(mainDir + 'DeepMile/assignment/OCEAN Summary Survey Results by Twitter UserName.csv', index_col='user_screen_name')\n",
      "regression_table = pd.merge(response, predictors, left_index=True, right_index=True)\n",
      "\n",
      "regression_table.to_csv(mainDir + '/DeepMile/regression_table.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:113: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}